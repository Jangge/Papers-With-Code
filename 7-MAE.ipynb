{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172728b8",
   "metadata": {},
   "source": [
    "# -----------------------------------搭建MAE的模型架构--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7640ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "# partial函数是一个非常实用的高阶函数，它用于创建一个新的可调用对象（如函数），这个新对象“部分地”固定了原函数的一些参数。\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "# patchEmbed:将二维的图像数据转换为一维的向量序列\n",
    "# Block：VIT的基本构建单元，包含自注意力层，多层感知机，归一化和激活函数等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65678ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Interpolate position embeddings for high-resolution\n",
    "# References:\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a732d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \n",
    "    \"\"\"1、搭建模型需要用到的默认层\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=1024,\n",
    "                 depth=24, num_heads=16, decoder_embed_dim=512, decoder_depth=8,\n",
    "                 decoder_num_heads=16, mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \"\"\"1.1定义编码器用到的层\"\"\"\n",
    "        \n",
    "        # 定义图像分割和转换操作\n",
    "        # img_size: 224*224输入图像大小 patch_size：16*16分割成块的大小，也就是分成了14块\n",
    "        # in_channs: 输入图像的通道数，embed_dim：嵌入维度，也就是原来矩形两维的图像块，被线性投影成1024维的\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        \n",
    "        # 一个图像分成patch的总数\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # 特殊起始标记cls，用于记住图片全局信息\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 使用nn.Parameter表示这是一个可以学习的参数\n",
    "        \n",
    "        # 固定的位置嵌入参数\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)\n",
    "        \n",
    "        # 直接调用BLOCK构建vit块\n",
    "        self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer) for i in range(depth)])\n",
    "        \n",
    "        # 定义规范化层\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        \"\"\"1.2定义解码器用到的层\"\"\"\n",
    "        # 全连接层\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        \n",
    "        # 用于填充或遮蔽位置的占位符\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        \n",
    "        # 解码器位置嵌入\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)\n",
    "        \n",
    "        # 使用vit块搭建解码器 qk_scale=None,这个报错\n",
    "        self.decoder_block = nn.ModuleList([Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, \n",
    "                                                  qkv_bias=True, norm_layer=norm_layer)\n",
    "                                           for i in range(decoder_depth)])\n",
    "        # 规范化层，用于模型规范输出\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        # 线形层，将输出重构为原来图像大小\n",
    "        self.deocder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True)\n",
    "        \n",
    "        \n",
    "        \"\"\"1.3其它\"\"\"\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    \"\"\"2、初始化权重的方法\"\"\"\n",
    "    def initialize_weights(self):\n",
    "        # 计算图片输入时的位置编码信息\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        # 将上一步计算结果转换为pytorch然后复制给模型的储存参数\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        # 计算解码器输入时需要的位置编码信息\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        # 获取patchembed层中类似线性层部分proj的权重参数，然后进行Xavier均匀初始化\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "        \n",
    "        # 对特殊的起始cls和掩蔽mask进行正态分布初始化，均值为0，标准差为0.02\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        \n",
    "        # 对所有线性层和norm层进行参数初始化操作，调用下面定义的方法\n",
    "        self.apply(self._init_weights) # 。apply会遍历模型所有子模块并应用传递给它的回调参数\n",
    "        \n",
    "    \"\"\"2.1定义线性层和归一化层的初始化参数\"\"\"\n",
    "    def _init_weights(self, m):\n",
    "        # 首先检查m是否是线性层\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # 是的话,使用xvaier初始化权重\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            # 加一个判断有无偏差的条件\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                # 将偏差初始化为0\n",
    "                # nn.init.constant_用于将张量的所有元素设置为给定的常数值\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        # 检查是否为归一化层\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            # 将归一化层的参数初始化\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        \n",
    "    \"\"\"3、将输入图像分割后再展平为一个新的张量\"\"\"\n",
    "    def patchify(self, imgs):\n",
    "        # 输入imgs:(N, 3, H, W) N是批量，3是通道， HW是高和宽\n",
    "        # 输出x：(N, L, patch_size**2*3) N是批量， L是分割成patch的数量，Patch...是将小patch转成一个向量\n",
    "        \n",
    "        # 获取patch的大小p\n",
    "        p = self.patch_embed.patch_size[0] \n",
    "        \n",
    "        # 断言图像为正方形并且能被patch整除\n",
    "        assert imgs.shape[2] == imges.shape[3] and imgs.shape[2] % p == 0\n",
    "        \n",
    "        # 为了获取patch数量的中间步骤\n",
    "        h = w = imgs.shape[2] // p\n",
    "        \n",
    "        # 调整张量形状\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        \n",
    "        # .einsum函数对张量进行重拍和展平，使每个patch先展成一维，然后连续储存\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        \n",
    "        # 改变张量形状\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2*3))\n",
    "        return x\n",
    "    \n",
    "    \"\"\"4、反patch化，将分割的patch重新组合成图像\"\"\"\n",
    "    def unpatchify(self, x):\n",
    "        # 输入x:(N, L, patch_size**2*3) 分割的patch\n",
    "        # 输出imgs:(N, 3, H, W)输出批量的图像\n",
    "        \n",
    "        # 获取patch的大小\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        # 反向求patch数量\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        # 断言patch数量一致\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        # 将patch组合成图像\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 2, h * p, h * p))\n",
    "        return imgs\n",
    "    \n",
    "    \"\"\"5、对输入序列进行随机掩码的操作\"\"\"\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        # 输入x为分割并处理后的图像，也就是patchify函数返回的值:(N, L, D)\n",
    "        # mask_ratio为掩码率？\n",
    "        \n",
    "        N, L, D = x.shape\n",
    "        \n",
    "        # 计算需要保留不掩避的序列长度\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        # 设置随机噪音张量\n",
    "        noise = torch.rand(N, L, device=x.device) \n",
    "        \n",
    "        # 用于实现样本随机打乱和恢复的操作\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "        \n",
    "        # 根据索引提取不被掩蔽的序列\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "        \n",
    "        # 建一个全为1的二进制掩码张量，形状为 [N, L]，然后将前 len_keep 个元素设置为0，表示这些位置是未被遮蔽的位置\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # 根据之前保存的恢复原顺序的索引 ids_restore，对掩码张量进行同样的操作，确保掩码张量与原始输入序列保持一致的顺序\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        \n",
    "        # 返回：经过掩码处理后的未被掩码部分的序列， 二进制掩码表示， 用于回复原始序列的索引\n",
    "        return x_mask, mask, ids_restore\n",
    "        \n",
    "    \"\"\"6、编码器的前向传播\"\"\"\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # 输入x:批量图片\n",
    "        \n",
    "        # 1-将输入的批量图片进行分割patch和转换\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # 2-添加位置编码\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        # 3-对输入图像进行掩蔽操作\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "        \n",
    "        # 4-添加cls特殊起始标记，用用于记录全局信息\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cles_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # 5-输入transform块进行计算\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        # 6-归一化计算\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 返回：编码器计算结果，而掩码标记mask，用于回复原始序列的索引\n",
    "        return x, mask, ids_restore\n",
    "    \n",
    "    \"\"\"7、解码器的前向传播\"\"\"\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        \n",
    "        # 1-将输入token转化为对应的高维向量表示\n",
    "        x = self.decoder_embed(x)\n",
    "        \n",
    "        # 2-创建于掩码数量相同的mask token向量\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        \n",
    "        # 3-将原始序列 x 中除第一个元素（即cls token）外的部分与刚创建的mask tokens拼接起来，形成一个新的序列 x_\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)\n",
    "        \n",
    "        # 4-反序列化\n",
    "        x_ = token.gather(x_, dim=1, index=ids_restore.unqueeze(-1).repeat(1, 1, x.shape[2]))\n",
    "        \n",
    "        # 5-将cls重新加入向量开头\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)\n",
    "        \n",
    "        # 6-添加位置编码\n",
    "        x = x + self.decoder_pos_embed\n",
    "        \n",
    "        # 7-输入解码器的transform块\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        # 8-归一化操作\n",
    "        x = self.decoder_norm(x)\n",
    "        \n",
    "        # 9-像素级预测层\n",
    "        x = self.decoder_pred(x)\n",
    "        \n",
    "        # 10-去除cls标记\n",
    "        x = x[:, 1:, :]\n",
    "        \n",
    "        # 返回的x和原来的输入图像是一样的\n",
    "        return x\n",
    "    \n",
    "    \"\"\"8、定义前向传播的损失loss\"\"\"\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        # 输入图像imgs:(N, 3, H, W) 批量-通道-高-宽\n",
    "        # 预测值（解码器的输出）pred(N, L, p*p*3) 批量-patch数量-每个patch所有像素的值\n",
    "        # 掩码表示mask:(N, L), 0是未掩码，1是掩码了的 批量-patch数量\n",
    "        \n",
    "        # 1-先将imgs转为patchs\n",
    "        target = self.patchify(imgs)\n",
    "        \n",
    "        # 2-对目标patch进行标准化处理，通过减去每patch各通道像素的均值并除以其方差（加了一个很小的数值以防止除零错误）来规范化数据\n",
    "        if norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "            \n",
    "        # 3-计算预测patch (pred) 与目标patch (target) 之间的平方误差损失，即每个样本每个patch的平均像素损失\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)\n",
    "        \n",
    "        # 4-将损失乘以掩码 mask 后求和，得到的是基于被移除patch的平均损失。\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \"\"\"9、定义模型前向传播\"\"\"\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        # 1-编码器前向传播\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        \n",
    "        # 2-解码器前向传播\n",
    "        pred = self.forward_decoder(latent, ids_restore)\n",
    "        \n",
    "        # 3-计算loss\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        \n",
    "        return loss, pred, mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731ec68",
   "metadata": {},
   "source": [
    "# 下面都是测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e8d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09106e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd1dc34",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [PatchEmbed: 1, Conv2d: 2, Identity: 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mD:\\miniconda\\envs\\machine_zzh\\lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\machine_zzh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[1;32mIn[3], line 259\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward\u001b[1;34m(self, imgs, mask_ratio)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs, mask_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m):\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# 1-编码器前向传播\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m     latent, mask, ids_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# 2-解码器前向传播\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 178\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward_encoder\u001b[1;34m(self, x, mask_ratio)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# 3-对输入图像进行掩蔽操作\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m x, mask, ids_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# 4-添加cls特殊起始标记，用用于记录全局信息\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 165\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.random_masking\u001b[1;34m(self, x, mask_ratio)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# 返回：经过掩码处理后的未被掩码部分的序列， 二进制掩码表示， 用于回复原始序列的索引\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx_mask\u001b[49m, mask, ids_restore\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_mask' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\machine_zzh\\lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m validate_user_params(\n\u001b[0;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[0;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    222\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[0;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[0;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    229\u001b[0m )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\machine_zzh\\lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [PatchEmbed: 1, Conv2d: 2, Identity: 2]"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59197d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a96dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedAutoencoderViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (decoder_block): ModuleList(\n",
       "    (0-7): 8 x Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (deocder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderViT2(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True,  norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True,  norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90be5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MaskedAutoencoderViT2(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c794e72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedAutoencoderViT2(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-7): 8 x Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1acf5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "326cfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d25b322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.mae_vit_base_patch16_dec512d8b(**kwargs)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_vit_base_patch16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342d9db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mae_vit_base_patch16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e93cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_zzh",
   "language": "python",
   "name": "machine_zzh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
